{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Tuple, Optional\n",
    "import torch.autograd as autograd\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# simple Module to normalize an image\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = torch.Tensor(mean)\n",
    "        self.std = torch.Tensor(std)\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean.type_as(x)[None,:,None,None]) / self.std.type_as(x)[None,:,None,None]\n",
    "\n",
    "# values are standard normalization for ImageNet images, \n",
    "# from https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "norm = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "\n",
    "device  = torch.device(\"cuda:0\")\n",
    "trn = transforms.Compose([\n",
    "#      transforms.Resize((224, 224)),\n",
    "     transforms.ToTensor(),])\n",
    "model = models.inception_v3(pretrained=True,transform_input=True).eval()\n",
    "# model = models.vgg19_bn(pretrained=True).eval()\n",
    "# model = models.alexnet(pretrained=True).eval()\n",
    "# model = models.resnet50(pretrained=True).eval()\n",
    "# model = models.densenet121(pretrained=True).eval()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "model.to(device)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CF(img, param,steps): #color filter\n",
    "        \n",
    "    param=param[:,:,None,None]\n",
    "    color_curve_sum = torch.sum(param, 4) + 1e-30\n",
    "    total_image = img * 0\n",
    "    for i in range(steps):\n",
    "        total_image += torch.clamp(img - 1.0 * i /steps, 0, 1.0 / steps) * param[:, :, :, :, i]\n",
    "    total_image *= steps/ color_curve_sum \n",
    "    return total_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACE-PGD with C&W loss\n",
    "batch_size=25\n",
    "max_iterations=100\n",
    "image_size=299\n",
    "steps=64 # the number of pieces\n",
    "bound=16# parameter bound\n",
    "\n",
    "lr=1 # learning_rate/step_size\n",
    "input_path='./images/'\n",
    "output_path='./ACE_PGD_'+str(steps)+'_lr_'+str(lr)+'_iter_'+str(max_iterations)+'_bound_'+str(bound)+'TET/'\n",
    "\n",
    "image_id_list = list(filter(lambda x: '.png' in x, os.listdir(input_path)))\n",
    "num_batches = np.int(np.ceil(len(image_id_list)/batch_size))\n",
    "iter_suc=np.zeros(10)  \n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "for k in tqdm_notebook(range(0,num_batches)):\n",
    "    loc_suc=np.ones(batch_size)*float('inf')# the minimal number of iterations for each image to success \n",
    "\n",
    "    batch_size_cur=min(batch_size,len(image_id_list)-k*batch_size)    \n",
    "    X_ori = torch.zeros(batch_size_cur,3,image_size,image_size).to(device)\n",
    "\n",
    "    for i in range(batch_size_cur):  \n",
    "        X_ori[i]=trn(Image.open(input_path+image_id_list[k*batch_size+i])).unsqueeze(0) \n",
    "\n",
    "    labels=torch.argmax(model(norm(X_ori)),dim=1)\n",
    "    labels_onehot = torch.zeros(labels.size(0), 1000, device=device)\n",
    "    labels_onehot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "    labels_infhot = torch.zeros_like(labels_onehot).scatter_(1, labels.unsqueeze(1), float('inf'))\n",
    "    Paras=torch.ones(batch_size_cur,3,steps).to(device)*1/steps\n",
    "    Paras.requires_grad=True            \n",
    "    prev = float('inf')\n",
    "    best_adversary = X_ori.clone()\n",
    "    for iteration in range(max_iterations):\n",
    "        X_adv = CF(X_ori, Paras,steps)  \n",
    "\n",
    "        logits = model(norm(X_adv))\n",
    "        real = logits.gather(1, labels.unsqueeze(1)).squeeze(1)\n",
    "        other = (logits - labels_infhot).max(1)[0]\n",
    "        loss = torch.clamp(real - other, min=0).sum()\n",
    "\n",
    "        loss.backward()\n",
    "        grad_a=Paras.grad.clone()\n",
    "\n",
    "        Paras.data=Paras.data-lr * (grad_a.permute(1,2,0)/(torch.norm(grad_a.view(batch_size_cur,-1),dim=1)+0.00000001)).permute(2,0,1)\n",
    "        Paras.grad.zero_()\n",
    "        Paras.data=torch.clamp(Paras.data,min=1/steps,max=1/steps*bound)\n",
    "\n",
    "        # early stop\n",
    "        if iteration % 25 == 0:\n",
    "            if loss > 0.9999*prev:\n",
    "                break\n",
    "            prev = loss\n",
    "\n",
    "        predicted_classes = (model(norm(X_adv))).argmax(1)\n",
    "        is_adv = (predicted_classes != labels)\n",
    "        best_adversary[is_adv] = X_adv[is_adv]\n",
    "        loc_suc[is_adv.cpu()]=np.minimum((np.ones(batch_size)*iteration)[is_adv.cpu()],loc_suc[is_adv.cpu()])\n",
    "\n",
    "    iter_suc=iter_suc+np.histogram(loc_suc,range=(0,max_iterations),bins=10)[0]\n",
    "    for j in range(batch_size_cur):\n",
    "        x_np=transforms.ToPILImage()(best_adversary[j].detach().cpu())\n",
    "        if labels[j]!=(model(norm(best_adversary)))[j].argmax(0):\n",
    "            x_np.save(os.path.join(output_path,image_id_list[k*batch_size+j]))\n",
    "        else:\n",
    "            x_np.save(os.path.join(output_path,image_id_list[k*batch_size+j][:-4]+'_fail.png'))\n",
    "for kk in range(1,10):\n",
    "    print(sum(iter_suc[0:kk]),end = ' ')#the number of successful adversarial images at different iteration cutoff.\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize original and adversarial images\n",
    "plt.subplot(121)\n",
    "plt.imshow((X_ori.data)[0].cpu().detach().numpy().transpose((1, 2, 0)),cmap=cm.gray)\n",
    "plt.subplot(122)\n",
    "plt.imshow((X_adv.data)[0].cpu().detach().numpy().transpose((1, 2, 0)),cmap=cm.gray)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
